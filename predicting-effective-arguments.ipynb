{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import the libraries \nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nimport transformers\nimport tqdm\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-17T12:37:20.334025Z","iopub.execute_input":"2022-08-17T12:37:20.334782Z","iopub.status.idle":"2022-08-17T12:37:28.938958Z","shell.execute_reply.started":"2022-08-17T12:37:20.334646Z","shell.execute_reply":"2022-08-17T12:37:28.937708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:28.941219Z","iopub.execute_input":"2022-08-17T12:37:28.941866Z","iopub.status.idle":"2022-08-17T12:37:29.320147Z","shell.execute_reply.started":"2022-08-17T12:37:28.941826Z","shell.execute_reply":"2022-08-17T12:37:29.318838Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"target_column = \"discourse_effectiveness\"\nle = preprocessing.LabelEncoder()\nle.fit(train_df[target_column])\ntrain_df['target'] = le.transform(train_df[target_column])","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.322026Z","iopub.execute_input":"2022-08-17T12:37:29.322514Z","iopub.status.idle":"2022-08-17T12:37:29.344591Z","shell.execute_reply.started":"2022-08-17T12:37:29.322454Z","shell.execute_reply":"2022-08-17T12:37:29.342928Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df['discourse_text']","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.348189Z","iopub.execute_input":"2022-08-17T12:37:29.349161Z","iopub.status.idle":"2022-08-17T12:37:29.359589Z","shell.execute_reply.started":"2022-08-17T12:37:29.349079Z","shell.execute_reply":"2022-08-17T12:37:29.358474Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import nltk ,re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string \nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\n\nnltk.download('wordnet')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.360814Z","iopub.execute_input":"2022-08-17T12:37:29.361333Z","iopub.status.idle":"2022-08-17T12:37:30.195294Z","shell.execute_reply.started":"2022-08-17T12:37:29.361268Z","shell.execute_reply":"2022-08-17T12:37:30.193877Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"emojis = \"🍕🐵😑😢🐶️😜😎👊😁😍💖💵👎😀😂🔥😄🏻💥😋👏😱🚌ᴵ͞🌟😊😳😧🙀😐😕👍😮😃😘💩💯⛽🚄😖🏼🚲😟😈💪🙏🎯🌹😇💔😡👌🙄😠😉😤⛺🙂😏🍾🎉😞🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪🐰🐇🐱🙆😨🙃💕💗💚🙈😴🏿🤗🇺🇸⤵🏆🎃😩👮💙🐾🐕😆🌠🐟💫💰💎🖐🙅⛲🍰🤐👆🙌💛🙁👀🙊🙉🚬🤓😵😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓😣⏺😌🤑🌏😯😲💞🚓🔔📚🏀👐💤🍇🏡❔⁉👠》🇹🇼🌸🌞🎲😛💋💀🎄💜🤢َِ🗑💃📣👿༼つ༽😰🤣🐝🎅🍺🎵🌎͟🤡🤥😬🤧🚀🤴😝💨🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦🍀😫🤤🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪🇫🇷🇩🇪😷🇨🇦🌐📺🐋💘💓💐🌋🌄🌅👺🐷🚶🤘ͦ💸👂👃🎫🚢🚂🏃👽😙🎾👹⎌🏒⛸🏄🐀🚑🤷🤙🐒🐈ﷻ🦄🚗🐳👇⛷👋🦊🐽🎻🎹⛓🏹🍷🦆♾🎸🤕🤒⛑🎁🏝🦁🙋😶🔫👁💲🗯👑🚿💡😦🏐🇰🇵👾🐄🎈🔨🐎🤞🐸💟🎰🌝🛳🍭👣🏉💭🎥🐴👨🤳🦍🍩😗🏂👳🍗🕉🐲🍒🐑⏰💊🌤🍊🔹🤚🍎𝑷🐂💅💢💒🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻🤖🎎😼🕷👼📉🍟🍦🌈🔭《🐊🐍🐦🐡💳ἱ🙇🥜🔼\"\n\ndef remove_emojis(text):\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:30.198508Z","iopub.execute_input":"2022-08-17T12:37:30.198922Z","iopub.status.idle":"2022-08-17T12:37:30.207555Z","shell.execute_reply.started":"2022-08-17T12:37:30.198888Z","shell.execute_reply":"2022-08-17T12:37:30.205539Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"wordnet_lemmatizer = WordNetLemmatizer()\nstopwords=stopwords.words('english')\nstemmer=PorterStemmer()\n# clean unwanted text like stopwords, @(Mention), https(url), #(Hashtag), punctuations\ndef removeUnwantedText(text):\n    #remove urls\n    if text == np.NaN or type(text) != str:\n      text = \" \"\n    text = re.sub(r'http\\S+', \" \", text)\n    text = re.sub(r'@\\w+',' ',text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub('r<.*?>',' ', text)\n    # html tags\n    text = text.lower()\n    text = text.split()\n    text = \" \".join([word for word in text if not word in stopwords])\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, \"\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:26.992594Z","iopub.execute_input":"2022-08-17T12:38:26.993145Z","iopub.status.idle":"2022-08-17T12:38:27.007366Z","shell.execute_reply.started":"2022-08-17T12:38:26.993097Z","shell.execute_reply":"2022-08-17T12:38:27.006185Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def basic_cleaning(text):\n    \"\"\"\n    clear url/ not alpha/ fuck-bitch swear\n    \"\"\"\n    text = re.sub(r'https?://www\\.\\S+\\.cm', '', text)\n    text = re.sub(r'[^a-zA-Z|\\s]', '', text)\n    text = re.sub(r'\\*+', 'swear', text)\n    return text\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    #emoticons\n    #symbols & pictographs\n    #transport & map symbols\n    #flags (iOS)\n    emoji_pattern = re.compile(\"[\"\\\n        u\"\\U0001F600-\\U0001F64F|\"\\\n        u\"\\U0001F300-\\U0001F5FF|\"\\\n        u\"\\U0001F680-\\U0001F6FF|\"\\\n        u\"\\U0001F1E0-\\U0001F1FF|\"\\\n        u\"\\U00002702-\\U000027B0|\"\\\n        u\"\\U000024C2-\\U0001F251\"\\\n        \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_multiplechars(text):\n    \"\"\"\n    for example, so we have “way” instead of “waaaayyyyy”\n    \"\"\"\n    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n    return text\n\ndef clean(df):\n    for col in ['discourse_text']:#,'selected_text']:\n        df[col] = df[col].astype(str).apply(lambda x:basic_cleaning(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_emoji(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_html(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n        df[col] = df[col].astype(str).apply(lambda x:removeUnwantedText(x))\n        df[col] = df[col].astype(str).apply(lambda x:removeUnwantedText(x))\n        df[col] = df[col].apply(lambda x: remove_emojis(x))\n    return df.sample(frac=1)\n\ntrain_df = clean(train_df)\ntrain_df_selection = train_df.sample(frac=1)\nX_tr = train_df_selection.discourse_text.values\n\n\ntest_df = clean(test_df)\ntest_df_selection = test_df.sample(frac=1)\nX_te = test_df_selection.discourse_text.values\nprint(X_te.shape)\nprint('clean Done')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:31.141058Z","iopub.execute_input":"2022-08-17T12:38:31.141486Z","iopub.status.idle":"2022-08-17T12:38:41.995137Z","shell.execute_reply.started":"2022-08-17T12:38:31.141450Z","shell.execute_reply":"2022-08-17T12:38:41.993801Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_df_selection","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:41.997074Z","iopub.execute_input":"2022-08-17T12:38:41.997526Z","iopub.status.idle":"2022-08-17T12:38:42.012778Z","shell.execute_reply.started":"2022-08-17T12:38:41.997493Z","shell.execute_reply":"2022-08-17T12:38:42.011327Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vocab_size = 16000  \nmaxlen = 64","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:42.014512Z","iopub.execute_input":"2022-08-17T12:38:42.014954Z","iopub.status.idle":"2022-08-17T12:38:42.023907Z","shell.execute_reply.started":"2022-08-17T12:38:42.014917Z","shell.execute_reply":"2022-08-17T12:38:42.023008Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing import sequence\n\ntokenizer = text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(train_df[\"text\"])\ndef prep_text(texts, tokenizer, max_sequence_length):\n    # Turns text into into padded sequences.\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:42.027067Z","iopub.execute_input":"2022-08-17T12:38:42.027719Z","iopub.status.idle":"2022-08-17T12:38:43.855358Z","shell.execute_reply.started":"2022-08-17T12:38:42.027669Z","shell.execute_reply":"2022-08-17T12:38:43.854255Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"x= prep_text(train_df['text'],tokenizer,maxlen)\nx= np.array(x)\ny =np.array(train_df['target'])\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20, random_state=4)\ny_val = tf.one_hot(y_val, 3)\ny_train= tf.one_hot(y_train, 3)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:43.856795Z","iopub.execute_input":"2022-08-17T12:38:43.857168Z","iopub.status.idle":"2022-08-17T12:38:45.499310Z","shell.execute_reply.started":"2022-08-17T12:38:43.857133Z","shell.execute_reply":"2022-08-17T12:38:45.498026Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:45.500821Z","iopub.execute_input":"2022-08-17T12:38:45.502178Z","iopub.status.idle":"2022-08-17T12:38:45.515378Z","shell.execute_reply.started":"2022-08-17T12:38:45.502127Z","shell.execute_reply":"2022-08-17T12:38:45.514325Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embed_dim = 64  # Embedding size for each token\nnum_heads = 4  # Number of attention heads\nff_dim = 64  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(3, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:45.516657Z","iopub.execute_input":"2022-08-17T12:38:45.518107Z","iopub.status.idle":"2022-08-17T12:38:45.947171Z","shell.execute_reply.started":"2022-08-17T12:38:45.518065Z","shell.execute_reply":"2022-08-17T12:38:45.945884Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:41:02.333028Z","iopub.execute_input":"2022-08-17T12:41:02.333444Z","iopub.status.idle":"2022-08-17T12:44:25.001812Z","shell.execute_reply.started":"2022-08-17T12:41:02.333409Z","shell.execute_reply":"2022-08-17T12:44:25.000500Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:16:10.450778Z","iopub.execute_input":"2022-08-17T12:16:10.451176Z","iopub.status.idle":"2022-08-17T12:19:44.113621Z","shell.execute_reply.started":"2022-08-17T12:16:10.451144Z","shell.execute_reply":"2022-08-17T12:19:44.112689Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"x_test = prep_text(test_df['discourse_type'],tokenizer,maxlen)\nx_test= np.array(x_test)\ntest_df[target_column] = le.inverse_transform(tf.argmax(model.predict(x_test), axis = 1).numpy())\ntest_df.to_csv(\"submission.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}