import nltk ,re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import string 
from nltk.stem import WordNetLemmatizer
import numpy as np

nltk.download('wordnet')
nltk.download('stopwords')

emojis = "ğŸ•ğŸµğŸ˜‘ğŸ˜¢ğŸ¶ï¸ğŸ˜œğŸ˜ğŸ‘ŠğŸ˜ğŸ˜ğŸ’–ğŸ’µğŸ‘ğŸ˜€ğŸ˜‚ğŸ”¥ğŸ˜„ğŸ»ğŸ’¥ğŸ˜‹ğŸ‘ğŸ˜±ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•ğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜ğŸ’©ğŸ’¯â›½ğŸš„ğŸ˜–ğŸ¼ğŸš²ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡ğŸ‘ŒğŸ™„ğŸ˜ ğŸ˜‰ğŸ˜¤â›ºğŸ™‚ğŸ˜ğŸ¾ğŸ‰ğŸ˜ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ªğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğŸ’—ğŸ’šğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸â¤µğŸ†ğŸƒğŸ˜©ğŸ‘®ğŸ’™ğŸ¾ğŸ•ğŸ˜†ğŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™ŒğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰ğŸš¬ğŸ¤“ğŸ˜µğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ğŸ˜²ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘ğŸ’¤ğŸ‡ğŸ¡â”â‰ğŸ‘ ã€‹ğŸ‡¹ğŸ‡¼ğŸŒ¸ğŸŒğŸ²ğŸ˜›ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙğŸ—‘ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°ğŸ¤£ğŸğŸ…ğŸºğŸµğŸŒÍŸğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§ğŸš€ğŸ¤´ğŸ˜ğŸ’¨ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦ğŸ€ğŸ˜«ğŸ¤¤ğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ˜·ğŸ‡¨ğŸ‡¦ğŸŒğŸ“ºğŸ‹ğŸ’˜ğŸ’“ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğŸ‘ºğŸ·ğŸš¶ğŸ¤˜Í¦ğŸ’¸ğŸ‘‚ğŸ‘ƒğŸ«ğŸš¢ğŸš‚ğŸƒğŸ‘½ğŸ˜™ğŸ¾ğŸ‘¹âŒğŸ’â›¸ğŸ„ğŸ€ğŸš‘ğŸ¤·ğŸ¤™ğŸ’ğŸˆï·»ğŸ¦„ğŸš—ğŸ³ğŸ‘‡â›·ğŸ‘‹ğŸ¦ŠğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸğŸğŸ¦ğŸ™‹ğŸ˜¶ğŸ”«ğŸ‘ğŸ’²ğŸ—¯ğŸ‘‘ğŸš¿ğŸ’¡ğŸ˜¦ğŸğŸ‡°ğŸ‡µğŸ‘¾ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ğŸ­ğŸ‘£ğŸ‰ğŸ’­ğŸ¥ğŸ´ğŸ‘¨ğŸ¤³ğŸ¦ğŸ©ğŸ˜—ğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²ğŸ’ğŸ‘â°ğŸ’ŠğŸŒ¤ğŸŠğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğŸ’¢ğŸ’’ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğŸ¤–ğŸğŸ˜¼ğŸ•·ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸğŸ¦ğŸ¡ğŸ’³á¼±ğŸ™‡ğŸ¥œğŸ”¼"

def remove_emojis(text):
    for emoji in emojis:
        text = text.replace(emoji, '')
    return text
    def removeUnwantedText(text):
    #remove urls
    if text == np.NaN or type(text) != str:
      text = " "
    text = re.sub(r'http\S+', " ", text)
    text = re.sub(r'@\w+',' ',text)
    text = re.sub(r'#\w+', ' ', text)
    text = re.sub('r<.*?>',' ', text)
    # html tags
    text = text.lower()
    text = text.split()
    text = " ".join([word for word in text if not word in stopwords])
    for punctuation in string.punctuation:
        text = text.replace(punctuation, "")
    return text
    
wordnet_lemmatizer = WordNetLemmatizer()
stopwords=stopwords.words('english')
stemmer=PorterStemmer()
# clean unwanted text like stopwords, @(Mention), https(url), #(Hashtag), punctuations
def removeUnwantedText(text):
    #remove urls
    if text == np.NaN or type(text) != str:
      text = " "
    text = re.sub(r'http\S+', " ", text)
    text = re.sub(r'@\w+',' ',text)
    text = re.sub(r'#\w+', ' ', text)
    text = re.sub('r<.*?>',' ', text)
    # html tags
    text = text.lower()
    text = text.split()
    text = " ".join([word for word in text if not word in stopwords])
    for punctuation in string.punctuation:
        text = text.replace(punctuation, "")
    return text
