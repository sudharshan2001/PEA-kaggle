{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import the libraries \nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nimport transformers\nimport tqdm\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-17T12:37:20.334025Z","iopub.execute_input":"2022-08-17T12:37:20.334782Z","iopub.status.idle":"2022-08-17T12:37:28.938958Z","shell.execute_reply.started":"2022-08-17T12:37:20.334646Z","shell.execute_reply":"2022-08-17T12:37:28.937708Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:28.941219Z","iopub.execute_input":"2022-08-17T12:37:28.941866Z","iopub.status.idle":"2022-08-17T12:37:29.320147Z","shell.execute_reply.started":"2022-08-17T12:37:28.941826Z","shell.execute_reply":"2022-08-17T12:37:29.318838Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"target_column = \"discourse_effectiveness\"\nle = preprocessing.LabelEncoder()\nle.fit(train_df[target_column])\ntrain_df['target'] = le.transform(train_df[target_column])","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.322026Z","iopub.execute_input":"2022-08-17T12:37:29.322514Z","iopub.status.idle":"2022-08-17T12:37:29.344591Z","shell.execute_reply.started":"2022-08-17T12:37:29.322454Z","shell.execute_reply":"2022-08-17T12:37:29.342928Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df['discourse_text']","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.348189Z","iopub.execute_input":"2022-08-17T12:37:29.349161Z","iopub.status.idle":"2022-08-17T12:37:29.359589Z","shell.execute_reply.started":"2022-08-17T12:37:29.349079Z","shell.execute_reply":"2022-08-17T12:37:29.358474Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import nltk ,re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string \nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\n\nnltk.download('wordnet')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:29.360814Z","iopub.execute_input":"2022-08-17T12:37:29.361333Z","iopub.status.idle":"2022-08-17T12:37:30.195294Z","shell.execute_reply.started":"2022-08-17T12:37:29.361268Z","shell.execute_reply":"2022-08-17T12:37:30.193877Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"emojis = \"ğŸ•ğŸµğŸ˜‘ğŸ˜¢ğŸ¶ï¸ğŸ˜œğŸ˜ğŸ‘ŠğŸ˜ğŸ˜ğŸ’–ğŸ’µğŸ‘ğŸ˜€ğŸ˜‚ğŸ”¥ğŸ˜„ğŸ»ğŸ’¥ğŸ˜‹ğŸ‘ğŸ˜±ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•ğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜ğŸ’©ğŸ’¯â›½ğŸš„ğŸ˜–ğŸ¼ğŸš²ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡ğŸ‘ŒğŸ™„ğŸ˜ ğŸ˜‰ğŸ˜¤â›ºğŸ™‚ğŸ˜ğŸ¾ğŸ‰ğŸ˜ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ªğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğŸ’—ğŸ’šğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸â¤µğŸ†ğŸƒğŸ˜©ğŸ‘®ğŸ’™ğŸ¾ğŸ•ğŸ˜†ğŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™ŒğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰ğŸš¬ğŸ¤“ğŸ˜µğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ğŸ˜²ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘ğŸ’¤ğŸ‡ğŸ¡â”â‰ğŸ‘ ã€‹ğŸ‡¹ğŸ‡¼ğŸŒ¸ğŸŒğŸ²ğŸ˜›ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙğŸ—‘ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°ğŸ¤£ğŸğŸ…ğŸºğŸµğŸŒÍŸğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§ğŸš€ğŸ¤´ğŸ˜ğŸ’¨ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦ğŸ€ğŸ˜«ğŸ¤¤ğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ˜·ğŸ‡¨ğŸ‡¦ğŸŒğŸ“ºğŸ‹ğŸ’˜ğŸ’“ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğŸ‘ºğŸ·ğŸš¶ğŸ¤˜Í¦ğŸ’¸ğŸ‘‚ğŸ‘ƒğŸ«ğŸš¢ğŸš‚ğŸƒğŸ‘½ğŸ˜™ğŸ¾ğŸ‘¹âŒğŸ’â›¸ğŸ„ğŸ€ğŸš‘ğŸ¤·ğŸ¤™ğŸ’ğŸˆï·»ğŸ¦„ğŸš—ğŸ³ğŸ‘‡â›·ğŸ‘‹ğŸ¦ŠğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸğŸğŸ¦ğŸ™‹ğŸ˜¶ğŸ”«ğŸ‘ğŸ’²ğŸ—¯ğŸ‘‘ğŸš¿ğŸ’¡ğŸ˜¦ğŸğŸ‡°ğŸ‡µğŸ‘¾ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ğŸ­ğŸ‘£ğŸ‰ğŸ’­ğŸ¥ğŸ´ğŸ‘¨ğŸ¤³ğŸ¦ğŸ©ğŸ˜—ğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²ğŸ’ğŸ‘â°ğŸ’ŠğŸŒ¤ğŸŠğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğŸ’¢ğŸ’’ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğŸ¤–ğŸğŸ˜¼ğŸ•·ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸğŸ¦ğŸ¡ğŸ’³á¼±ğŸ™‡ğŸ¥œğŸ”¼\"\n\ndef remove_emojis(text):\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:37:30.198508Z","iopub.execute_input":"2022-08-17T12:37:30.198922Z","iopub.status.idle":"2022-08-17T12:37:30.207555Z","shell.execute_reply.started":"2022-08-17T12:37:30.198888Z","shell.execute_reply":"2022-08-17T12:37:30.205539Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"wordnet_lemmatizer = WordNetLemmatizer()\nstopwords=stopwords.words('english')\nstemmer=PorterStemmer()\n# clean unwanted text like stopwords, @(Mention), https(url), #(Hashtag), punctuations\ndef removeUnwantedText(text):\n    #remove urls\n    if text == np.NaN or type(text) != str:\n      text = \" \"\n    text = re.sub(r'http\\S+', \" \", text)\n    text = re.sub(r'@\\w+',' ',text)\n    text = re.sub(r'#\\w+', ' ', text)\n    text = re.sub('r<.*?>',' ', text)\n    # html tags\n    text = text.lower()\n    text = text.split()\n    text = \" \".join([word for word in text if not word in stopwords])\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, \"\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:26.992594Z","iopub.execute_input":"2022-08-17T12:38:26.993145Z","iopub.status.idle":"2022-08-17T12:38:27.007366Z","shell.execute_reply.started":"2022-08-17T12:38:26.993097Z","shell.execute_reply":"2022-08-17T12:38:27.006185Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def basic_cleaning(text):\n    \"\"\"\n    clear url/ not alpha/ fuck-bitch swear\n    \"\"\"\n    text = re.sub(r'https?://www\\.\\S+\\.cm', '', text)\n    text = re.sub(r'[^a-zA-Z|\\s]', '', text)\n    text = re.sub(r'\\*+', 'swear', text)\n    return text\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    #emoticons\n    #symbols & pictographs\n    #transport & map symbols\n    #flags (iOS)\n    emoji_pattern = re.compile(\"[\"\\\n        u\"\\U0001F600-\\U0001F64F|\"\\\n        u\"\\U0001F300-\\U0001F5FF|\"\\\n        u\"\\U0001F680-\\U0001F6FF|\"\\\n        u\"\\U0001F1E0-\\U0001F1FF|\"\\\n        u\"\\U00002702-\\U000027B0|\"\\\n        u\"\\U000024C2-\\U0001F251\"\\\n        \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_multiplechars(text):\n    \"\"\"\n    for example, so we have â€œwayâ€ instead of â€œwaaaayyyyyâ€\n    \"\"\"\n    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n    return text\n\ndef clean(df):\n    for col in ['discourse_text']:#,'selected_text']:\n        df[col] = df[col].astype(str).apply(lambda x:basic_cleaning(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_emoji(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_html(x))\n        df[col] = df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n        df[col] = df[col].astype(str).apply(lambda x:removeUnwantedText(x))\n        df[col] = df[col].astype(str).apply(lambda x:removeUnwantedText(x))\n        df[col] = df[col].apply(lambda x: remove_emojis(x))\n    return df.sample(frac=1)\n\ntrain_df = clean(train_df)\ntrain_df_selection = train_df.sample(frac=1)\nX_tr = train_df_selection.discourse_text.values\n\n\ntest_df = clean(test_df)\ntest_df_selection = test_df.sample(frac=1)\nX_te = test_df_selection.discourse_text.values\nprint(X_te.shape)\nprint('clean Done')","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:31.141058Z","iopub.execute_input":"2022-08-17T12:38:31.141486Z","iopub.status.idle":"2022-08-17T12:38:41.995137Z","shell.execute_reply.started":"2022-08-17T12:38:31.141450Z","shell.execute_reply":"2022-08-17T12:38:41.993801Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_df_selection","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:41.997074Z","iopub.execute_input":"2022-08-17T12:38:41.997526Z","iopub.status.idle":"2022-08-17T12:38:42.012778Z","shell.execute_reply.started":"2022-08-17T12:38:41.997493Z","shell.execute_reply":"2022-08-17T12:38:42.011327Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vocab_size = 16000  \nmaxlen = 64","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:42.014512Z","iopub.execute_input":"2022-08-17T12:38:42.014954Z","iopub.status.idle":"2022-08-17T12:38:42.023907Z","shell.execute_reply.started":"2022-08-17T12:38:42.014917Z","shell.execute_reply":"2022-08-17T12:38:42.023008Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing import sequence\n\ntokenizer = text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(train_df[\"text\"])\ndef prep_text(texts, tokenizer, max_sequence_length):\n    # Turns text into into padded sequences.\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:42.027067Z","iopub.execute_input":"2022-08-17T12:38:42.027719Z","iopub.status.idle":"2022-08-17T12:38:43.855358Z","shell.execute_reply.started":"2022-08-17T12:38:42.027669Z","shell.execute_reply":"2022-08-17T12:38:43.854255Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"x= prep_text(train_df['text'],tokenizer,maxlen)\nx= np.array(x)\ny =np.array(train_df['target'])\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20, random_state=4)\ny_val = tf.one_hot(y_val, 3)\ny_train= tf.one_hot(y_train, 3)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:43.856795Z","iopub.execute_input":"2022-08-17T12:38:43.857168Z","iopub.status.idle":"2022-08-17T12:38:45.499310Z","shell.execute_reply.started":"2022-08-17T12:38:43.857133Z","shell.execute_reply":"2022-08-17T12:38:45.498026Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:45.500821Z","iopub.execute_input":"2022-08-17T12:38:45.502178Z","iopub.status.idle":"2022-08-17T12:38:45.515378Z","shell.execute_reply.started":"2022-08-17T12:38:45.502127Z","shell.execute_reply":"2022-08-17T12:38:45.514325Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embed_dim = 64  # Embedding size for each token\nnum_heads = 4  # Number of attention heads\nff_dim = 64  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(3, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:38:45.516657Z","iopub.execute_input":"2022-08-17T12:38:45.518107Z","iopub.status.idle":"2022-08-17T12:38:45.947171Z","shell.execute_reply.started":"2022-08-17T12:38:45.518065Z","shell.execute_reply":"2022-08-17T12:38:45.945884Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:41:02.333028Z","iopub.execute_input":"2022-08-17T12:41:02.333444Z","iopub.status.idle":"2022-08-17T12:44:25.001812Z","shell.execute_reply.started":"2022-08-17T12:41:02.333409Z","shell.execute_reply":"2022-08-17T12:44:25.000500Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-17T12:16:10.450778Z","iopub.execute_input":"2022-08-17T12:16:10.451176Z","iopub.status.idle":"2022-08-17T12:19:44.113621Z","shell.execute_reply.started":"2022-08-17T12:16:10.451144Z","shell.execute_reply":"2022-08-17T12:19:44.112689Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"x_test = prep_text(test_df['discourse_type'],tokenizer,maxlen)\nx_test= np.array(x_test)\ntest_df[target_column] = le.inverse_transform(tf.argmax(model.predict(x_test), axis = 1).numpy())\ntest_df.to_csv(\"submission.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}